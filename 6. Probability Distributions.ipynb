{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Discrete vs. Continuous Probability Distributions](#Discrete-vs.-Continous-Distributions)\n",
    "- [Probability Intervals & Histograms](#Probability-Intervals-&-Histograms)\n",
    "- [Probability Density](#Probability-Density)\n",
    "- [Normal Distribution](#Normal-Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete vs. Continuous Probability Distributions\n",
    "\n",
    "So far we have covered probability distributions as functions that map probabilities to each of the possible outcomes of a random variable. The sum of all the probabilities of a probability distribution must equal 1 (as they include 100% of the possible outcomes). However, depending on the properties of the random variable we are looking at, the mathematical tools that we looked at before cannot directly be applied.\n",
    "\n",
    "Consider the example we covered in the __\"Probability\"__ notebook with the dart throw where the random variable was the area where the dart landed. There were four distinct, countable possible outcomes for the random variable. This is known as a __discrete random variable__, and follows a __discrete probability distribution__, where there is a __finite, countable number of possible outcomes__. This means that we can assign probabilities to each of the outcomes. Since for the dart throw example, we had a discrete probability distribution, we were able to assign individual probabilities to each individual outcome.\n",
    "\n",
    "However, not all all random variables follow a _discrete_ distribution. What if I chose to investigate a random variable $X$, representing the height of a randomly selected student in a classroom. If I have a perfect ruler that measures with perfect accuracy, how many possible heights are there that can be observed? While perhaps not intuitive at first, the answer is that there are __infinitely many options__. \n",
    "\n",
    "This is not necessarily to do with treating a height of 0cm or a height of $\\infty$cm as possibilities. One way to look at this is that if I pick any two possible heights, there is always a possible height in between:\n",
    "- If I pick the heights 169cm and 170cm, someone can still be 169.5cm\n",
    "- If I pick the heights 169cm and 169.5cm, someone can still be 169.25cm\n",
    "- If I pick the heights 169cm and 169.25cm, someone can still be 169.125cm\n",
    "- If I pick the heights 169cm and 169.125cm, someone can still be 169.0625cm\n",
    "- $\\vdots$\n",
    "\n",
    "And we can carry out this process indefinitely. This means that even within a small range, we have infinitely many possible values our random variable can take. This is known as a __continuous random variable__, and follows a __continuous probability distribution__, where there is an __infinite, uncountable number of possible outcomes__. One more visual approach to be able to distinguish the two random variables is shown in the diagram below:\n",
    "<img src=\"images/jumps.png\" height=\"500px\" width=\"500px\"/>\n",
    "\n",
    "From this diagram it becomes clearer that a continuous random variable can be thought of as a continuous, smooth, connected range of possible values, whereas for a discrete random variable, we have a countable set of possible 'points' or outcomes. If we go back to the example of our dart throw, our random variable had four possible outcomes: $X \\in \\{1, 2, 3, 4\\}$. If we pick two possible outcomes, let's say 1 and 2, we see that there are no possible outcomes in between. This further confirms that it follows a discrete distribution.\n",
    "\n",
    "\n",
    "But how does the distinction between discrete and continuous random variables affect the tools we apply to given problems? Well, we saw from the beginning that the sum of the probabilities of all outcomes must equal 1. However, in this case, there are infinitely many possible outcomes. What kind of issue does this lead to?\n",
    "\n",
    "\n",
    "Since the sum of infinitely many positive numbers, no matter how big, is also infinity. Therefore, the individual probability of each outcome must be zero. But wait! Didn't we say the sum of the probabilities must be one? Then how does the sum of infinitely many zeros equal 1?!\n",
    "\n",
    "<img src=\"https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/160/facebook/105/shocked-face-with-exploding-head_1f92f.png\"/>\n",
    "\n",
    "This means that __there are inherent differences in the representation of continuous and discrete probability distributions.__ With continuous random variables, it does not make sense to assign a probability to a single outcome. If we cannot look at the likelihood of individual possible outcomes, how do we study them in the context of probability?\n",
    "\n",
    "One way to analyse the likelihood of values of continuous random variables is that instead of looking at individual values, we consider the __likehood of a value being within a given range or interval__. This leads us to a crucial tool to analyse continuous probability distributions: the __histogram__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Intervals & Histograms\n",
    "\n",
    "Given a dataset from a given random variable, we can construct a __histogram__, where we split the range of observed values into consecutive, non-overlapping intervals, referred to as __bins__ or __buckets__, measure the number of times a data point falls into each given bin, and plot this in a bar chart. The number of times an event occurs is known as its __frequency__. If we think of our data points as lying along a given number line, we can understand the concept of a histogram as follows:\n",
    "\n",
    "<img src=\"https://plot.ly/static/img/literacy/fig1.gif\" />\n",
    "<img src=\"https://plot.ly/static/img/literacy/fig2.gif\" />\n",
    "\n",
    "Let us now try to apply the concept of a histogram to the example of the height of students in a classroom. Below, we create a function that generates a given number of data points for this example, then use our own histogram generator based on Plotly Bar Charts. We will discuss what _normal_ means in more detail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import utils\n",
    "\n",
    "def class_heights(n):\n",
    "    return np.random.normal(175, 4, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = class_heights(n)\n",
    "bins, frequencies = utils.get_freq_data(x_lower=160, x_upper=190, data=x, bin_width=5)\n",
    "fig = utils.get_hist(bins=bins, frequencies=frequencies)\n",
    "fig.update_layout(\n",
    "    title_text='Height Distribution', # title of plot\n",
    "    xaxis_title_text='Height(cm)', # xaxis label\n",
    "    yaxis_title_text='Count (Frequency)', # yaxis label\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted above the histogram for the height distribution of a sample of 100 students, with a constant bin width of 5cm throughout. Some of you may be thinking: \"How does this weird looking thing relate to probability?\". Well, from our initial definition, probability is \"the number of wanted outcomes divide by the number of total outcomes\". Therefore, if we divide the frequency of each bin by the total number of outcomes, we get the __relative frequency__, which is the probability of a data point falling within the given bin.\n",
    "\n",
    "In our example, if we divide the number of students in each bin by 100 (total number of students), we get the following histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_data(x_lower, x_upper, data, bin_width):\n",
    "    bins, frequencies = utils.get_freq_data(x_lower, x_upper, data, bin_width)\n",
    "    probabilities = frequencies/np.sum(frequencies) ##\n",
    "    return bins, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, probabilities = get_prob_data(160, 190, x, 5)\n",
    "fig = utils.get_hist(bins, probabilities)\n",
    "fig.update_layout(\n",
    "    title_text='Height Distribution', # title of plot\n",
    "    xaxis_title_text='Height(cm)', # xaxis label\n",
    "    yaxis_title_text='Probability', # yaxis label\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that these are the correct probabilities corresponding to the different intervals by checking that the sum of all probabilities is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sum = np.sum(probabilities)\n",
    "print(prob_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the answer computed by Python is not exactly 1. The reason for this is known as numerical underflow, which can lead to small deviations from the result of floating point operations. We can check that the result we got is 1 with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isclose(1, prob_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now given set of data points sampled from a continuous random variable, we can describe the probability of given intervals using histograms. Therefore, this histogram is an __estimate of the probability distribution of the random variable we are investigating__. This is why histograms are an incredibly useful tool for understanding the underlying distribution of data we have sampled. Just as when we dealt with probability, we can calculate the probability of a wider interval by adding the probability of different intervals. For instance:\n",
    "\n",
    "$$P(170<X\\leq180) = P(170<X\\leq175) + P(175<X\\leq180)$$\n",
    "$$P(160<X\\leq180) = P(160<X\\leq165) + P(165<X\\leq170) + P(170<X\\leq175) + P(175<X\\leq180)$$\n",
    "\n",
    "\n",
    "However, this is not a perfect way to analyse probability. Why do you think that is?\n",
    "\n",
    "Well, not only are we only __estimating__ the probability distribution given a dataset, but we are turning a continuous random variable into a discrete random variable by grouping a range of values together. It is now discrete since if we look at different bins of a histogram, there are no bins in between. For instance, for the plot above, there are no bins between (170-175cm) and (175-180)cm. This means that __when the size of the bins are too large, we are losing a lot of information about the underlying distribution!__ For example, by looking at the above histogram, we have no way of differentiating between the probability distribution of measuring 171cm or 173cm, since they are in the same bin. We also have no way of measuring _all_ possible intervals. For example, we are unable to measure the $P(173<X<176)$. How do we go about fixing these problems?\n",
    "\n",
    "\n",
    "Well, if the problem occurs when the bins are too large, we can just make bins smaller right? This may have been unfeasible centuries ago, but with the power of programming we can make our bin width SIGNIFICANTLY small. However, as we saw above, since this is an estimate, the more data the more the estimate approaches its true value. This is even more relevant when we decrease bin width, as this means we have _more bins,_ meaning we need more data _per bin._\n",
    "\n",
    "Let us assume that we are sampling 100,000 students, meaning the probability distribution estimate tends to the true probability distribution, then see the effect of decreasing bin width on the quality of our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = class_heights(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Code\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "utils.visualize_prob_hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we approach a reasonably large number of bins, the probability of each bin approaches zero, as we see in the histograms above. This is because, as we the bin width gets closer and closer to 0, the more the 'interval' we are looking at starts behaving like a 'point'. However, we can see that even though individual probabilities get smaller as we decrease the width our intervals, __the probabilities remain the same relative to each other__. This is reflected by how the _shape_ of the plots remain relatively similar. How can we use this finding to better describe the probability distribution of this continuous random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density\n",
    "\n",
    "The optimal solution to this problem is to _normalize_ our probabilties with respect to the length of their interval (bin width), which gives us what is known as the __probability density__ of that interval, given by the equation below:\n",
    "\n",
    "$$ \\text{Probability Density} = \\frac{\\text{Interval Probability}}{\\text{Interval Width}}$$\n",
    "\n",
    "This explains why most histograms will use the same bin width for all bins. When we do this, we get the histograms shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_data(x_lower, x_upper, data, bin_width):\n",
    "    bins, probabilities = get_prob_data(x_lower, x_upper, data, bin_width)\n",
    "    prob_densities = probabilities/bin_width ##\n",
    "    return bins, prob_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_density_hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, even at incredibly small bin widths, we are still getting plots on the same scale while preserving the relevant information regarding the underlying probability distribution. In the case of our example, probability density is measured as __probability per cm__. \n",
    "\n",
    "The most important property of probability density is that as the interval width gets incredibly small (pretty much 0), such that each interval can be approximated by a point, we can still measure probability density (unlike probability)! Knowing the interval width (bin width), we can also easily calculate the probability of an interval given its probability density by rearranging the equation above:\n",
    "\n",
    "$$ \\text{Interval Probability}= \\text{Probability Density} \\times \\text{Interval Width}$$\n",
    "\n",
    "Coincidentally, if we look at our probability density histograms, we see that this corresponds to the area of the bar corresponding to that interval, as shown in the diagram below:\n",
    "\n",
    "<img src=\"images/area.png\" width=\"500px\" height=\"500px\"/>\n",
    "\n",
    "Therefore, if we have an estimate of the probability density distribution, we can estimate the probability of any given interval. In theory, if we know the probability density of every single point, we can calculate the exact probability of any given interval. While in the real-world, we will never know the _exact_ probability density distribution of a continuous random variable, we can model them with what are known as __probability density functions (PDFs)__. These are functions that define the probability density of every single possible outcome of a continuous random variable.\n",
    "\n",
    "Any function can be treated as a probability density function as long as it meets the two following criteria:\n",
    "- The probabilities described by the PDF must equal to 1\n",
    "- All probability densities must be positive\n",
    "\n",
    "The concept of PDFs is generally quite abstract, so it becomes more intuitive to think of them as __probability density histograms with lots of data and really, really small interval widths.__ For example, let us assume that a continuous random variable, X, follows the PDF given below:\n",
    "\n",
    "$$f(x) = e^{-x}$$\n",
    "\n",
    "Let us assume that we were able to collect 10,000 samples from this continuous random variable. Below, we plot this function with the respective histogram at different interval widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data points to plot the given function\n",
    "def exp_pdf(x):\n",
    "    return np.exp(-x) ##\n",
    "x_true = np.linspace(0, 5, 1000)\n",
    "f_true = exp_pdf(x_true)\n",
    "\n",
    "# Sampling the exponential distribution given above\n",
    "n = 100000\n",
    "x_sample = np.random.exponential(size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_exp_approx(x_sample, x_true, f_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, for a reasonably large set of data points, as we make our bin width smaller and smaller, we approach the true PDF. Now that you have this intuitive understanding of PDFs, how can we calculate the probability of an interval of values for our continuous random variable?\n",
    "\n",
    "Since the area of the rectangular bar representing the interval is the probability of that interval, we just __add the probability of all the intermediary intervals__. As the bin widths get smaller and smaller, meaning we get closer to the true PDF, this becomes the area underneath the curve. Therefore, for any PDF, __we can calculate the probability of any interval as the area underneath the curve within that interval.__ Below we show a function that computes the probability of an interval given the true PDF, which allows us to tweak the bin width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_estimate(x_lower, x_upper, pdf_func, bin_width):\n",
    "    area_count = 0\n",
    "    current_x = x_lower\n",
    "    while current_x < x_upper:\n",
    "        area_count+= pdf_func(current_x)*bin_width ## adding the area of the current interval\n",
    "        current_x += bin_width ##\n",
    "    return area_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_exp_interval(x_true, f_true, exp_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this histogram approximation allows us to compute the probability of an interval of values with accuracy increasing as we decrease the interval width. Therefore, if you have to investigate the probability distribution of any continuous random variable, you can either:\n",
    "- Estimate the probability/probability density of individual intervals of values by sampling the variable using histograms for the data (e.g. how we collected data from a large classroom)\n",
    "- Use functions to model the probability density of a continuous random variable at every point (as we did when looking at the exponential distribution)\n",
    "\n",
    "As we can expect, since the sum of the probabilities of all possible outcomes must be one, __the entire area under the curve of a pdf must equal 1__. While we cannot show this exactly for $f(x) = e^{-x}$, as it goes on forever, we can estimate it with a large enough number (in this case 10,000), as shown below for a bin width of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_estimate(0, 10000, exp_pdf, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, now you know the intuition behind the probability distribution of both discrete and continuous variables! We will now go over what is possibly the most important continuous probability distribution: the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "\n",
    "The __normal distribution__ is a continuous probability distribution that is commonly used to approximate the probability distribution of continuous random varaibles whose true distributions are not known. Given its shape, it is also informally referred to as a __bell curve__, although there are other less common bell-shaped distributions out there. It has a probability density function given by the following function:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}}$$\n",
    "\n",
    "Where $\\sigma$ is the population standard deviation and $\\mu$ the population mean of the continuous random variable x. We generally denote a normally distributed random variable as shown below:\n",
    "\n",
    "$$X \\sim N(\\mu, \\sigma ^{2})$$\n",
    "\n",
    "We can create a Python function to model it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mean, std):\n",
    "    exponent = ((x-mean)/std)**2\n",
    "    exponent *= -0.5\n",
    "    output = np.exp(exponent)/(std*np.sqrt(2*np.pi))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 12\n",
    "std_store = [0.1, 0.5, 1, 2]\n",
    "x_vals = np.linspace(8, 16, 1000)\n",
    "colors = [\"orange\", \"red\", \"blue\", \"green\"]\n",
    "fig = go.Figure()\n",
    "for std, color in zip(std_store, colors):\n",
    "    y_current = normal(x_vals, mean, std)\n",
    "    fig.add_trace(go.Scatter(x=x_vals, y=y_current,\n",
    "                             marker_color=color, \n",
    "                             name='$\\sigma = {s}$'.format(s=std))\n",
    "                 )\n",
    "fig.update_yaxes(title_text=\"Probability Density\")\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "\n",
    "print(\"Mean: \", mean)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it is clear that __the standard deviation determines the _spread_ of the probability distribution and the mean determines the _centre_ location of the distribution.__ This makes sense, as the mean being a measure of central tendency, is the most likely value to occur, and the further away from it, the less likely values are to occur. The same logic can be applied to the standard deviation: if you, on average, are further away from the mean (hence a larger standard deviation), there is a larger probability of observing values further from the mean. An imporant subsection of this is known as the __standard normal distribution__, which follows the distribution $X \\sim N(0, 1)$. It will covered in much more detail in Chapter 2. This is plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter values\n",
    "mean = 0\n",
    "std = 1\n",
    "x_vals = np.linspace(-3, 3, 1000)\n",
    "y_vals = normal(x_vals, mean, std)\n",
    "\n",
    "# Plotting figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, marker_color=\"orange\"))\n",
    "fig.update_yaxes(title_text=\"Probability Density\")\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_layout(title_text=\"Standard Normal Distribution\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep in mind: _\"Normality is a myth; there never was, and never will be, a normal distribution\" (Robert Geary, 1947)._ As we mentioned before, PDFs are only models we assume about our variables to try and compute given probabilities. Nevertheless, it may still be extremely useful. For instance, if a continuous random variable has higher probabilities near its mean and lower probabilities away from the mean, we may be able to _approximate_ the underlying distribution as normal. There are multiple methods out there to verify how well we can model a variable with a normal distribution. We will not go over them in the scope of this chapter, but a simple way of verifying this is through the use of a histogram of the sampled data. As expected, the more data and more bins we use, the better we can verify this. \n",
    "\n",
    "In the previous example of the measured height of students, when we assumed we were able to gather 100,000 samples, we got the following histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = class_heights(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_density_hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this (ideal) case we can see that our continuous variable seems to be well-modelled by a normal distribution.\n",
    "\n",
    "Another, perhaps even more useful property of the normal distribution comes about through what is known as the __central limit theorem (CLT).__ The CLT formally states that, if we have $n$ observations, given that all observations were generated from the same distribution, the mean of these observations will follow (approximately) a normal distribution given that $n$ is large enough. The larger the number of observations, the better the approximation.\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}$$\n",
    "\n",
    "Where $X_{1},...,X_{n}$ are the $n$ observations and $\\bar{X}$ is their mean.\n",
    "\n",
    "A common rule thumb is that the CLT can be applied when $n>35$. Not only will it have a normal distribution, but this distribution will have the following properties:\n",
    "\n",
    "$$\\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n})$$\n",
    "\n",
    "How does this actually work? Well, if we assume that all our random variables follow the same, identical distribution, then they must have the same mean. For the example of the heights of students, we can treat each student as an observation following the same, unknown distribution. However, in practice, we will never have infinitely many data points, and rarely even have 100,000. This means that depending on which students we measure the height from, __we only have an estimate of the true (population) mean__, which has its own mean and standard deviation.\n",
    "\n",
    "For example, let us look once more at the exponential distribution we looked at above. Below, we sample 20 points from a variable with this distribution and then take the mean. This process is carried out 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling an exponential distribution\n",
    "n = 20\n",
    "for i in range(5):\n",
    "    x_sample = np.random.exponential(size=n)\n",
    "    print(np.mean(x_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from here, even though each set of 20 samples follows the same exponential distribution, the mean shows fluctuactions across the different sets, which means it must have a __standard deviation__. How does this standard deviation change with respect to the number of data points in each set we average? We check this out below for 4 different numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = [5, 10, 20, 100]\n",
    "\n",
    "for n in n_vals:\n",
    "    mean_set = np.array([])\n",
    "    for i in range(1000):\n",
    "        x_sample = np.random.exponential(size=n)\n",
    "        mean_set = np.append(mean_set, np.mean(x_sample))\n",
    "    print(\"n:\", n, \", std:\", np.std(mean_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, the more data we have, the better our estimates become, which means the more data we have, the smaller the variance and standard deviation of the estimate become, justifying why the variance of the calculated sample mean is scaled by $\\frac{1}{n}$. But how do we know it follows a normal distribution if n is large enough? We will show this for the exponential distribution above via simulations.\n",
    "\n",
    "Let us say that we collect $n$ samples from a variable that follows the exponential distribution, and find the mean of these samples. Let us also carry out this procedure 1000 more times. We now have 1000 sample means, which we can use to construct histogram estimates of the underlying distribution as we saw before! Let us do this below for different values of n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original distribution\n",
    "x_true = np.linspace(0, 5, 1000)\n",
    "f_true = exp_pdf(x_true)\n",
    "\n",
    "# Sample mean distribution\n",
    "n_vals = [2, 20, 50, 200]\n",
    "mean_sets = np.array([])\n",
    "for n in n_vals:\n",
    "    mean_set = np.array([])\n",
    "    for i in range(1000):\n",
    "        x_sample = np.random.exponential(size=n)\n",
    "        mean_set = np.append(mean_set, np.mean(x_sample))\n",
    "    mean_sets = np.append(mean_sets, mean_set)\n",
    "\n",
    "mean_sets = np.reshape(mean_sets, (4, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting original distribution\n",
    "utils.visualize_exp_true(x_true, f_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting estimated distributions of sample mean\n",
    "utils.visualize_mean_approx(n_vals, mean_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the histograms above, when the number of observations is small, the estimated distribution is not a good fit, but as we increase the number of samples, our distribution appears more and more similar to a normal distribution and the associated standard deviation decreases! This is an incredibly powerful tool, as it works for __any unknown distribution.__ The CLT and the ability to aproximate certain random variables as having a normal distribution makes it incredibly useful in data science and statistics. We will cover many of its uses throughout the course.\n",
    "\n",
    "The final question then becomes: how do we find the probability of an interval of a normal distribution? Let us go back to the example of the height of students. Since we generated it using a normal distribution, we know it has $\\mu = 175cm$ and $\\sigma = 4cm$. Let us find out what is the probability of selecting a student with a height greater than 180cm ($P(X > 180)$)? Below we plot the normal PDF and the interval which we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student height distribution\n",
    "def student_normal(x):\n",
    "    return normal(x, 175, 4)\n",
    "\n",
    "n = 100000\n",
    "bw = 0.1\n",
    "interval_floor = 180\n",
    "interval_ceil = 200\n",
    "x_sample = class_heights(n)\n",
    "utils.visualize_normal_interval(x_sample, n, interval_floor, interval_ceil, student_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the PDF tells us the probability density at every point (tiny intervals), the probability is given by the area under the curve of that interval. We can use the sum of many tiny bars from our histogram to approximate this probability, which is what our function _prob\\_estimate(  )_ does. Below, we get the appropriate probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob_estimate(interval_floor, interval_ceil, student_normal, bw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means we only expect about 10.8% of students to be taller than 180cm. We can approximate this from our sample by dividing the number of students taller than 180cm by the total number students, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall_count = 0\n",
    "for x_point in x_sample:\n",
    "    if x_point > 180:\n",
    "        tall_count += 1 ##\n",
    "print(tall_count/len(x_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have now mastered probability distributions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
